{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "A Data set containing m_train and test  images labeled as cat(y=1) or non-cat(y=0)<br>\n",
    "\n",
    "Each image is of shape (num_px, num_px, 3)<br>\n",
    "\n",
    "\n",
    "#### Build a simple image-recognistion algorithm that can correctly classify pictures as cat or non-cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"_orig\" is added because we will be pre-processing the data set\n",
    "\n",
    "Each line of train_set_x_orig and test_set_x_orig is an array representing an image. Wecan visualize an example by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize the dataset.\n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the parts of algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we use Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "   \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the parameters (w and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0.0\n",
    "    \n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function for forward and backward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X)+b)                              # compute activation\n",
    "    cost = -1/m * (np.sum(Y * np.log(A) + (1-Y) * (np.log(1-A))))  # compute cost\n",
    "    \n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    \n",
    "    dz= (1/m)*(A - Y)\n",
    "    dw = np.dot(X,dz.T)\n",
    "    db = np.sum(dz)\n",
    "    \n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final setep is optimisation using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation \n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "       \n",
    "        w = w - (learning_rate*dw)\n",
    "        b = b - (learning_rate*db)\n",
    "       \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we define predict function for predicting the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    \n",
    "    A =  sigmoid(np.dot(w.T,X)+ b)\n",
    " \n",
    "    #\n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        Y_prediction = 1. * (A > 0.5)\n",
    "        \n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge all these function to give a final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    \n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the model for the given data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train accuracy is very high but test accuracy is low. that is model is overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply a deep neural network and see how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build<br>\n",
    "\n",
    "l layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Structure of l layer Neural Net  is as follows: <br>\n",
    "\n",
    "1) Initialize the parameters for ùêø -layer neural network<br>\n",
    "2) Implement the forward propagation module<br>\n",
    "3) Complete the LINEAR part of a layer's forward propagation step (resulting in  ùëç[ùëô] ).<br>\n",
    "4) The ACTIVATION function (relu/sigmoid)<br>\n",
    "5) Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.<br>\n",
    "6) Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer  ùêø ). This gives a new L_model_forward function.<br>\n",
    "7) Compute the loss<br>\n",
    "8) Implement the backward propagation module <br>\n",
    "9) Complete the LINEAR part of a layer's backward propagation step<br>\n",
    "10) The gradient of the ACTIVATE function (relu_backward/sigmoid_backward)<br>\n",
    "11) Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function<br>\n",
    "12) Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function<br>\n",
    "13) Finally, update the parameters<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we bulid the linear part of the forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define a the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        \n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        \n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a l-layer Neural Net we will need a function that replicates the previous (linear_activation forward with ReLU) L-1 times , then follows that with one linear_activation_forward with sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b' + str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "       \n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "   \n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    \n",
    "    logprobs = np.multiply(np.log(AL), Y) + np.multiply((1 - Y), np.log(1 - AL))\n",
    "    cost = -1/m*np.sum(logprobs)\n",
    "    \n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    \n",
    "    dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "    db =  1/m*(np.sum(dZ,axis=1, keepdims=True))\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "   \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear-Activation Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L-Model Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,\"sigmoid\")\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    \n",
    "    for i in range(1,L+1):\n",
    "        parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - learning_rate*grads[\"dW\"+str(i)]\n",
    "        parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - learning_rate*grads[\"db\"+str(i)]\n",
    "        \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will put all these in one function to finally get a l-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0025, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        \n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        \n",
    "        \n",
    "        # Compute cost.\n",
    "        \n",
    "        cost = compute_cost(AL,Y)\n",
    "        \n",
    "    \n",
    "        # Backward propagation.\n",
    "        \n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        \n",
    " \n",
    "        # Update parameters.\n",
    "        \n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "       \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "            \n",
    "      # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()      \n",
    "    \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply this model to our data set and see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] # 5 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.687463\n",
      "Cost after iteration 200: 0.682446\n",
      "Cost after iteration 300: 0.678019\n",
      "Cost after iteration 400: 0.674110\n",
      "Cost after iteration 500: 0.670659\n",
      "Cost after iteration 600: 0.667610\n",
      "Cost after iteration 700: 0.664917\n",
      "Cost after iteration 800: 0.662537\n",
      "Cost after iteration 900: 0.660433\n",
      "Cost after iteration 1000: 0.658572\n",
      "Cost after iteration 1100: 0.656925\n",
      "Cost after iteration 1200: 0.655468\n",
      "Cost after iteration 1300: 0.654178\n",
      "Cost after iteration 1400: 0.653035\n",
      "Cost after iteration 1500: 0.652022\n",
      "Cost after iteration 1600: 0.651125\n",
      "Cost after iteration 1700: 0.650329\n",
      "Cost after iteration 1800: 0.649623\n",
      "Cost after iteration 1900: 0.648997\n",
      "Cost after iteration 2000: 0.648441\n",
      "Cost after iteration 2100: 0.647948\n",
      "Cost after iteration 2200: 0.647510\n",
      "Cost after iteration 2300: 0.647120\n",
      "Cost after iteration 2400: 0.646774\n",
      "Cost after iteration 2500: 0.646467\n",
      "Cost after iteration 2600: 0.646194\n",
      "Cost after iteration 2700: 0.645951\n",
      "Cost after iteration 2800: 0.645735\n",
      "Cost after iteration 2900: 0.645542\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x, train_set_y, layers_dims, num_iterations = 3000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555023923444976\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x, train_set_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3400000000000001\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train and test accuracy is very poor. Should work through it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will have to use bigger network and large train time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
